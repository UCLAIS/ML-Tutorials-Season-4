{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtGu_ZWNNNyl"
      },
      "source": [
        "# Challenge - Clickbait Title Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItvJEtA6NNym"
      },
      "source": [
        "# Background information\n",
        "\n",
        "Clickbait titles and thumbnails are plagueing the internet and lead to lesser user satisfaction with services like YouTube or news servers. Due to the amount on new content on these sites, it is impossible to control content manually. That is why giants like Facebook(Meta), Twitter, Amazon or Google(Alphabet) are investing huge resources towards creating NLP systems that ae able to curate internet enviroment autonomously.\n",
        "\n",
        "To make our Clickbait Detection model we will use Bag of Words encoding and sequential model.\n",
        "\n",
        "# Data\n",
        "\n",
        "We will use clickbait data, which you can download from our GitHub.It has 2 categories (\"headline\" - containing titles & clickbait - containing the labels). As the separator, we use \";\" because comma can be problematic on some system due to commas being also used in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Q17QHbNNyn",
        "outputId": "850dff54-eb98-4a51-f849-30b587c806a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "# Importing required libraries and download NLTK resources\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zo4tOBvcNNyo"
      },
      "outputs": [],
      "source": [
        "# Load data into dataframe\n",
        "DATA_PATH = 'clickbait_data.csv'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.dropna(subset = [\"clickbait\"], inplace=True)\n",
        "np.random.shuffle(df.values)\n",
        "\n",
        "# Load corpus\n",
        "corpus = list(df[\"headline\"])\n",
        "labels = array(df[\"clickbait\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooX_yLqPNNyo"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "In NLP there are multiple ways how to approach preprocessing. It is more or less up to us, what kinds of preprocessing we want to do and not always are all of them helpful.\n",
        "The most common preprocessing techniques are:\n",
        "- Removing stopwords\n",
        "- Lemmatizaton\n",
        "- Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2sCff5vNNyp",
        "outputId": "482472d1-3bf5-43a9-baf2-9422f21bbd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18538\n"
          ]
        }
      ],
      "source": [
        "# Get all unique words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "all_words = []\n",
        "for sent in corpus:\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    for word in tokenize_word:\n",
        "        if word not in stopwords:\n",
        "            word = stemmer.stem(word)\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            all_words.append(word)\n",
        "unique_words = set(all_words)\n",
        "print(len(unique_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tsk239hNNyp"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "Creating embeddings could be also seen as a form of preprocessing, which is maybe the most important choice you make when building NLP model. We are using the Bag of Words approach, which is very simplistic. They are better embeddings for this task, but there are situation when BoW is the best option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Y2L7a48NNyp"
      },
      "outputs": [],
      "source": [
        "# Create embeddings\n",
        "vocab_length = len(unique_words) + 5\n",
        "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "padded_sentences = pad_sequences(embedded_sentences, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fJIJ2wbzNNyp"
      },
      "outputs": [],
      "source": [
        "#Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_test, labels_train, labels_test = train_test_split(embedded_sentences, labels, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3CEfIZ_5NNyq"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 20, input_length=padded_sentences.shape[1]))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SMkJUr_gNNyq"
      },
      "outputs": [],
      "source": [
        "#Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5H_LfWxNNyq",
        "outputId": "b426ed5a-f21b-49d6-92a0-6ab182d87094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "700/700 [==============================] - 5s 8ms/step - loss: 0.0051 - acc: 0.9996\n",
            "Epoch 2/5\n",
            "700/700 [==============================] - 6s 8ms/step - loss: 0.0030 - acc: 0.9998\n",
            "Epoch 3/5\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.0018 - acc: 0.9999\n",
            "Epoch 4/5\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 5/5\n",
            "700/700 [==============================] - 6s 8ms/step - loss: 6.9430e-04 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a7131d82410>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#Fit model\n",
        "model.fit(data_train, labels_train, epochs=5, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFQCIdkoNNyq"
      },
      "outputs": [],
      "source": [
        "#Count accuracy\n",
        "loss, accuracy = model.evaluate(data_test, labels_test, verbose=0,batch_size=10)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}