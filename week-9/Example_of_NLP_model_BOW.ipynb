{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Clickbait Title Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background information\n",
    "\n",
    "Clickbait titles and tumbnails are plagueing the internet and lead to lesser user satisfaction with services like YouTube or news servers. Due to the amount on new content on these sites, it is impossible to control content manually. That is why giants like Facebook(Meta), Twitter, Amazon or Google(Alphabet) are investing huge resources towards creating NLP systems that ae able to curate internet enviroment autonomously.\n",
    "\n",
    "To make our Clickbait Detection model we will use Bag of Words encoding and sequential model.\n",
    "\n",
    "# Data\n",
    "\n",
    "We will use clickbait data, which you can download from our GitHub.It has 2 categories (\"headline\" - containing titles & clickbait - containing the labels). As the separator, we use \";\" because comma can be problematic on some system due to commas being also used in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries and download NLTK resources\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into dataframe\n",
    "DATA_PATH = 'train.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.dropna(subset = [\"clickbait\"], inplace=True)\n",
    "np.random.shuffle(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load corpus \n",
    "corpus = list(df[\"headline\"])\n",
    "labels = array(df[\"clickbait\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In NLP there are multiple ways how to approach preprocessing. It is more or less up to us, what kinds of preprocessing we want to do and not always are all of them helpful.\n",
    "The most common preprocessing techniques are:\n",
    "- Removing stopwords\n",
    "- Lemmatizaton\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all unique words\n",
    "lemmatizer = WordNetLemmatizer();\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "all_words = []\n",
    "for sent in corpus:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    for word in tokenize_word:\n",
    "        if word not in stopwords:\n",
    "            if word not in stopwords:\n",
    "                word = stemmer.stem(word)\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                all_words.append(word)\n",
    "unique_words = set(all_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Creating embeddings could be also seen as a form of preprocessing, which is maybe the most important choice you make when building NLP model. We are using the Bag of Words approach, which is very simplistic. They are better embeddings for this task, but there are situation when BoW is the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embeddings\n",
    "vocab_length = len(unique_words)+5\n",
    "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(padded_sentences, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit model\n",
    "model.fit(data_train, labels_train, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count accuracy\n",
    "loss, accuracy = model.evaluate(data_test, labels_test, verbose=0,batch_size=10)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
