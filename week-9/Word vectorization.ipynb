{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5b6f79",
   "metadata": {},
   "source": [
    "# 2. Word Vectorization\n",
    "\n",
    "So far we have learned techniques and procedures for spplitting sentences / paragraphs to individual words, reducing them to word base and removing non-relevant words. On the other hand, these words could not be used as a model input they are still in the textual format.\n",
    "\n",
    "**We need to convert input data from its raw textual format into vectors of real numbers**. This step is also known as **vectorization**.\n",
    "\n",
    "In a way, vectorization can be imagined as a feature extraction step as the main idea evolves around getting distinct features out of the text in order to train our model.\n",
    "\n",
    "There are plenty of different algorithms and pre-written models that can be used to vectorize textual data, however, in this tutorial we will only look at **Bag of Words** and **TF-IDF**.\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "It is one of the simplest techniques out there which involves three steps:\n",
    "\n",
    "1. **Tokenization**. We need to convert text to list of sequences.\n",
    "\n",
    "2. **Creating vocabulary**. To create vocabulary, we extract uniques words from the whole word list and then sort them by alphabetical order.\n",
    "\n",
    "3. **Creating vector**. After extracting the frequency at which each word vocabulary appears in the initial word list, we construct a matrix. In this sparse matrix, each row is a sentence vector whose length is equal to the size of vocabulary.\n",
    "\n",
    "Without any further or do, let's look at one of the possible implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd91794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "Text = \"Natural language processing (NLP) refers to the branch of computer science. To be more specific, the branch of artificial intelligence. It is concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\"\n",
    "\n",
    "#Sentence tokenization\n",
    "sent_tokenize = sent_tokenize(Text)\n",
    "\n",
    "#Creating CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform(sent_tokenize)\n",
    "\n",
    "#Converting to array for visualization purposes\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574be315",
   "metadata": {},
   "source": [
    "Let's print our vocabulary to better understand our matrix structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093fee4",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "- Every row represents sentence extracted from the initial text\n",
    "- The length of each column is equal to the length of vocabulary\n",
    "- Each value represents word's frequency in the sentence\n",
    "\n",
    "As you can see, Bag of Words is a very simple, yet effective algorithm that can be applied to many cases. On the other hand, it simplicity causes some limitations. Since BoW model is only concerned with the frequency of vocabulary, articles, prepositions, and conjunctions get the same importance as other words, even though, they do not actually carry the same 'weight'. This is where the **TF-IDF** algorithm comes in.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency is a numarical statistic that is intended to evaluate the importance of the word in the given textual sample.\n",
    "\n",
    "TF-IDF algorithm has two parts:\n",
    "- **Term Frequency**. It can be understood as a normalized frequency score (it's value will always be smaller than 1) that can be calculated using the following formula:\n",
    "![TF](https://lh4.googleusercontent.com/qeUw5Ui1H-nvG-CjoSiJ7rA3vuiRN-YNRVNzY0rCroT36t71R36ZRckGdQAOzQdC7iXlBge8qlLcjtE2aITLRLoXrx56eEs2ucNtyEaSm6X5xXSutWB1ckUHkg6ScBxFUYJqVdTc)\n",
    "\n",
    "\n",
    "- **Inverse Document Frequency**. Document Frequency (DF) itself shows the proportion of documents that contain a certain word. If we inverse DF and tage the logarithm, we get the expresion for IDF:\n",
    "![IDF](https://lh5.googleusercontent.com/M5Zkpe89P6lDf4x__CzwQPIMkG7jJ7ediYWVkJYBsv1-eslTAfcCw0zbl5_2xrn3p2sRTgd-budDlwzjgj4lyny-WO5KLGIDosPRRMEj4zR_fNbl5SWEwF0Xm1m8UOK7pTOs6Zxz)\n",
    "\n",
    "Overall, the more common the word is accross all documents, the lesser its importance is for the current document. The final TF-IDF score can be found as follows:\n",
    "\n",
    "![TF-IDF](https://lh5.googleusercontent.com/LjP1udnGPjJHZ-qlkeO02ToZM21OykkOflxmi3cCVAljxHPpV1jf5PFHIwjkXoCKjAWMn1WB9Ln5fAcypxWppCsSlkorQYr0TZ1rGNYCfXNjHCWhM1RqwQTV8B0fRMtp_bAKjm5A)\n",
    "\n",
    "Now, let's look how all of this can be implemented in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ab251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "Text = \"Natural language processing (NLP) refers to the branch of computer science. To be more specific, the branch of artificial intelligence. It is concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\"\n",
    "\n",
    "#Sentence tokenization\n",
    "sent_tokenize = sent_tokenize(Text)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "transformed = tfidf.fit_transform(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ffe1c",
   "metadata": {},
   "source": [
    "To see which features are the most important, let's construct a data frame with feature names and sorted TF-IDF scores as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(transformed[0].T.todense(),\n",
    "    index=tfidf.get_feature_names(), columns=[\"Score\"])\n",
    "df = df.sort_values('Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f379e",
   "metadata": {},
   "source": [
    "Despite still being simple, TF-IDF is widely used in tasks where model has to evaluate the best response to a query, especially in a chatbot system or keyword extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
