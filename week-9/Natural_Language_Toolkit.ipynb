{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d41525f",
      "metadata": {
        "id": "6d41525f"
      },
      "source": [
        "# 1. Natural Language Toolkit\n",
        "\n",
        "### What is Natural Language Toolkit?\n",
        "\n",
        "Natural Language Toolkit (or NLTK for short) is a group of libraries and programs used for symbolic and statistical natural language processing.\n",
        "\n",
        "As it has been mentioned previously, data for NLP model has to be preprocessed prior to the training procedure. Such preprocessing operations could include, converting string-type data to numerical data, performing semantical analysis, etc. All of these (and many more) operations can be simply implemented using the NLTK library.\n",
        "\n",
        "In the following section, we will look at the most relevant functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a34cf6a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a34cf6a7",
        "outputId": "49a36ed9-753b-4fcf-812a-71d34ac96b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "#Setting up\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "90c9c926",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90c9c926",
        "outputId": "edcdbdec-5432-45b4-fa31-513c233cf5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc33c09",
      "metadata": {
        "id": "6dc33c09"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Prior to processing textual data, we should first tokenize it. In other words, we should split it into smaller parts (sentences to words, paragraphs to sentences), as it reduces further processing time.\n",
        "\n",
        "#### Sentence tokenization\n",
        "\n",
        "As the name might suggest, in the sentence tokenization we aim to split groups of sentences/paragraphs to shorter sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d96c6a2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d96c6a2e",
        "outputId": "a2c6290c-77a0-4057-d0d5-448e3fc7930f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing (NLP) refers to the branch of computer science.',\n",
              " 'To be more specific, the branch of artificial intelligence.',\n",
              " 'It is concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "Text = \"Natural language processing (NLP) refers to the branch of computer science. To be more specific, the branch of artificial intelligence. It is concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\"\n",
        "\n",
        "sent_tokenize = sent_tokenize(Text)\n",
        "\n",
        "sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68776a8e",
      "metadata": {
        "id": "68776a8e"
      },
      "source": [
        "#### Word tokenization\n",
        "\n",
        "In contrast to sentence tokenization, the goal of word tokenization is to divide textual data into individual words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "53e91104",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53e91104",
        "outputId": "87e6e905-c966-4881-f4ff-d755a88e421b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'the',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'computer',\n",
              " 'science',\n",
              " '.',\n",
              " 'To',\n",
              " 'be',\n",
              " 'more',\n",
              " 'specific',\n",
              " ',',\n",
              " 'the',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'giving',\n",
              " 'computers',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'text',\n",
              " 'and',\n",
              " 'spoken',\n",
              " 'words',\n",
              " 'in',\n",
              " 'much',\n",
              " 'the',\n",
              " 'same',\n",
              " 'way',\n",
              " 'human',\n",
              " 'beings',\n",
              " 'can',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "word_tokenize = nltk.word_tokenize(Text)\n",
        "\n",
        "word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e931ccc",
      "metadata": {
        "id": "2e931ccc"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "![stemming](https://media.licdn.com/dms/image/C4D12AQEZCHQOHXSEhg/article-cover_image-shrink_600_2000/0/1650689035153?e=2147483647&v=beta&t=uRsPEF-Apt9EvVTcUGR_ZhAs_Dk39de4MFQDo78LHos)\n",
        "\n",
        "Stemming is the process of producing morphological variants using the given base word. The usage of stemming algorithms allows multiple word variations to share a *same* meaning / attribute (for instance, 'fish', 'fishing' are variants of the base word 'fish').\n",
        "\n",
        "In many cases, stemming algorithm tends to cut the end of the word until the base word is found, as it works in the most cases.\n",
        "\n",
        "Let's look at one of the most common stemming tools implementation - **Porter Stemmer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "184b8ef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184b8ef4",
        "outputId": "0100bdde-cd97-43c0-b707-6bd75cf4545d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fishing-----fish\n",
            "believes-----believ\n",
            "writes-----write\n",
            "loving-----love\n",
            "cats-----cat\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['fishing', 'believes', 'writes', 'loving', 'cats']\n",
        "\n",
        "for word in words:\n",
        "    print(word + '-----' + stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72330617",
      "metadata": {
        "id": "72330617"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "In contrast to stemming, lemmatization does not apply simple *'word end cutting'* and rather considers a full vocabulary to apply a morphological word analysis. The simple examples of such reduction could be interpretting 'was' as one of the 'be' or 'mice' as 'mouse'.\n",
        "\n",
        "As a result, lemmatization is considered to be more informative than simple stemming, but also, slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "25f6d0b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25f6d0b8",
        "outputId": "874613da-1c26-4789-91ec-9ab79757661c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "believes-----belief\n",
            "lives-----life\n",
            "mice-----mouse\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer();\n",
        "\n",
        "words = ['believes', 'lives', 'mice']\n",
        "\n",
        "for word in words:\n",
        "    print(word + '-----' + lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04e69b8",
      "metadata": {
        "id": "f04e69b8"
      },
      "source": [
        "On the other hand, same words can be interpreted differently based on the part of speech. To correctly interpret such words, we can specify the ```pos``` argument in the lemmatizer function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5579ede0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5579ede0",
        "outputId": "7cd7718b-d1df-4f35-ed3e-cc5ecc220234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crossing\n",
            "cross\n"
          ]
        }
      ],
      "source": [
        "#crossing as adjective\n",
        "print(lemmatizer.lemmatize('crossing', pos = 'a'))\n",
        "\n",
        "#crossing as verb\n",
        "print(lemmatizer.lemmatize('crossing', pos = 'v'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40307ab",
      "metadata": {
        "id": "d40307ab"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Stopwords are the words in any language which do not add much meaning to a sentence, thus can be ignored without lossing accuracy. The examples of such words could be 'is', 'the', 'at', however, each NLP tool has a different list of stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ebf58500",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf58500",
        "outputId": "107ab5e1-d9e9-4bba-bc2a-2f516d045772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c25a7f2",
      "metadata": {
        "id": "3c25a7f2"
      },
      "source": [
        "As these words do not carry much information, they can be removed, however, this depends from case to case.\n",
        "\n",
        "Generally, NLP models for text classification, sentiment analysis, spam classification (and so on) would require the removal of stop words.\n",
        "\n",
        "On the other hand, if we are dealing with the translation models, stopwords should be left as they might provide some contextual information.\n",
        "\n",
        "Let's remove stopwords from the previously analyzed NLP definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "333ca929",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333ca929",
        "outputId": "f4352f9a-dcc1-449b-901b-c2eb9d9690fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'branch', 'computer', 'science', '.', 'To', 'specific', ',', 'branch', 'artificial', 'intelligence', '.', 'It', 'concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'much', 'way', 'human', 'beings', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "Text = \"Natural language processing (NLP) refers to the branch of computer science. To be more specific, the branch of artificial intelligence. It is concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\"\n",
        "\n",
        "words = nltk.word_tokenize(Text)\n",
        "\n",
        "filtered_words = []\n",
        "\n",
        "for word in words:\n",
        "    if word not in stopwords:\n",
        "        filtered_words.append(word)\n",
        "\n",
        "print(filtered_words)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}